{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import json\n",
    "\n",
    "print(os.listdir())\n",
    "print(os.listdir('unzipped'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "contents = os.listdir()\n",
    "# Should list among others: data_zipped, elasticserch-1.7.1, unzipped\n",
    "# data_zipped should contain all hep-th-YYYY.tar.gz ziles\n",
    "# unzipped should either be empty or filled with directories per year\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    base = 'http://www.cs.cornell.edu/projects/kddcup/download/hep-th-'\n",
    "    filebase = 'hep-th-'\n",
    "    datafiles = [str(n) for n in range(1992, 2004)]\n",
    "    for df in datafiles:\n",
    "        testfile = urllib.request.URLopener()\n",
    "        testfile.retrieve(base + df + '.tar.gz', 'data_zipped/' + filebase + df +'.tar.gz')\n",
    "\n",
    "    \n",
    "if 'unzipped' not in contents:\n",
    "    os.mkdir('unzipped')\n",
    "if 'data_zipped' not in contents:\n",
    "    os.mkdir('data_zipped')\n",
    "    if not len(os.listdir('data_zipped')):\n",
    "        get_dataset()\n",
    "if 'json' not in contents:\n",
    "    os.mkdir('json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "zipped_folder = os.path.join(os.path.curdir, 'data_zipped')\n",
    "\n",
    "\n",
    "def extract_tar():\n",
    "    unzipped_folder = os.path.join(os.path.curdir, 'unzipped')\n",
    "    datafiles = [str(n) for n in range(1991, 2004)]\n",
    "    for tf in os.listdir(zipped_folder):\n",
    "        tfi = os.path.join(zipped_folder, tf) # tarfile instance\n",
    "        if True:\n",
    "            print(tfi)\n",
    "            if (tfi.endswith(\"tar.gz\")):\n",
    "                tar = tarfile.open(tfi, \"r:gz\")\n",
    "                tar.extractall()\n",
    "                tar.close()\n",
    "            elif (tfi.endswith(\"tar\")):\n",
    "                tar = tarfile.open(tfi, \"r:\")\n",
    "                tar.extractall()\n",
    "                tar.close()\n",
    "        shutil.move(tfi[-11:-7], os.path.join(unzipped_folder,tf[:-7]))\n",
    "        \n",
    "if not os.listdir('unzipped'):\n",
    "    extract_tar()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = 'unzipped/hep-th-2003/0301005'\n",
    "latex_lines = []\n",
    "\n",
    "with open(os.path.join(os.path.curdir, testfile)) as f:\n",
    "    #print(''.join(f.readlines()))\n",
    "    latex_lines = f.readlines()\n",
    "\n",
    "#print(latex_lines)\n",
    "print('\\n'.join([line[:-1] for line in latex_lines]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def strip_comments(ll):\n",
    "    \"\"\" returns lines of the latex file, except that comments, i.e. everything behind % on a\n",
    "    \n",
    "    line, are removed.\n",
    "    \"\"\"\n",
    "    line_list = ll[:]\n",
    "    for ind, line in enumerate(line_list):\n",
    "        comment_pos = line.find('%')\n",
    "        if comment_pos != -1:\n",
    "            line_list[ind] = line[:line.find('%')]\n",
    "        else:\n",
    "            line_list[ind] = line[:-1]\n",
    "    line_list = [line.strip() for line in line_list]\n",
    "    line_list = [line for line in line_list if line]\n",
    "    return line_list\n",
    "            \n",
    "\n",
    "    \n",
    "def get_name_section(section_string, depth):\n",
    "    \"\"\" returns (sub) section name, string\n",
    "    \n",
    "    :param section_string: string, latex_line with section tag on it\n",
    "    :param depth: int, depth of section 0 = \\section, 1 = \\subsection etc.\n",
    "    :return: string, either what is included in the section tag, or all of it that is included on that line\n",
    "    \n",
    "    # TODO:\n",
    "      -  check for closing tag beforehand, such that complete names can be extracted, if the name spans multiple rows\n",
    "      -  simplify function by replacing the string in find for start with levelname(depth) (works now but tired, so not risking)\n",
    "    \"\"\"\n",
    "    start = section_string.find('\\\\' + 'sub'* (depth) + 'section{')\n",
    "    end = section_string.find('}')\n",
    "    truestart = start + 3*depth + 9 #adding the latex command to index\n",
    "    return section_string[truestart:end]\n",
    "\n",
    "\n",
    "def levelname(depth):\n",
    "    \"\"\" returns string with appropriate (sub) section latex command for given depth\n",
    "    \n",
    "    :param depth: int\n",
    "    \"\"\"\n",
    "    return '\\\\' + 'sub'*depth + 'section{'\n",
    "\n",
    "def section_delimiters(linelist, level):\n",
    "    \"\"\" returns two lists, delimiters for (sub) sections as indices of the list,\n",
    "    \n",
    "    and their accompanying names\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    names = []\n",
    "    lvln = levelname(level)\n",
    "    for i, line in enumerate(linelist):\n",
    "        if line.find(lvln) != -1:\n",
    "            indices.append(i)\n",
    "            names.append(get_name_section(line, level))\n",
    "    return indices + [-1], names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \"\"\" the basic unit that will be dictionaryfied, currently each node containstheir own name (section title), \n",
    "    \n",
    "    a list of subnodes and accompanying names.\n",
    "    in the future the headnode (in the parsetree) will function as the main node, containing \n",
    "    subnodes 'recursively', as well as being the location in which other keywords and values are placed\n",
    "    (i.e. 'author':[authorlist], 'date':'date' (in multiple formats for search))\n",
    "\n",
    "    each of these keyswords gives more options to the user later on, so if \n",
    "    you have some 'free' time... (ha wouldn't you wish)\n",
    "    this is a TODO as well\n",
    "    \n",
    "    I emphasize: if this is not done well, our search is going to seriously suck.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, name, linelist, level, headnode=False):\n",
    "        self.tot_lines = len(linelist)\n",
    "        self.headnode = headnode\n",
    "        self.name = name\n",
    "        self.linelist = linelist\n",
    "        leveldelimiters, childnames =  section_delimiters(linelist, level)\n",
    "        self.ld, self.cnames = leveldelimiters, childnames\n",
    "        self.cn = []\n",
    "        #print(level, self.name)\n",
    "\n",
    "        if headnode:\n",
    "            self.other_keys = {}\n",
    "            tempkeys = KEYTAGS[:]\n",
    "            tempkeynames = [tk[0] for tk in tempkeys]\n",
    "            for ind, line in enumerate(linelist):\n",
    "                name, content = self.extract_tags(line, ind, tempkeys)\n",
    "                if name:\n",
    "                    temp_n_ind = tempkeynames.index(name)\n",
    "                    tempkeys.pop(temp_n_ind)\n",
    "                    tempkeynames.pop(temp_n_ind)\n",
    "                self.other_keys[name] = content.translate(PUNCTUATION_TABLE)\n",
    "            #for k,v in self.other_keys.items():\n",
    "                #print(k, v, \"\\n\")\n",
    "\n",
    "            \n",
    "\n",
    "    def extract_tags(self, line, ind, tempkeys):\n",
    "        \"\"\"\n",
    "        \n",
    "        KEYTAGS have format [[\"name\", \"latex_begin_tag\", \"latex_end_tag\"], ...]\n",
    "        \"\"\"\n",
    "        found_lbt = \"\"\n",
    "        found_let = \"\"\n",
    "        found_n = \"\"\n",
    "        close = 0\n",
    "        content = \"\"\n",
    "        for i, keytag in enumerate(tempkeys):\n",
    "            n, comb = keytag\n",
    "            for lbt, let in comb:\n",
    "                i = line.find(lbt)\n",
    "\n",
    "                if i != -1:\n",
    "                    #print(lbt, line)\n",
    "                    try:\n",
    "                        close = self.find_closing(let, ind)\n",
    "                    except IndexError:\n",
    "                        print(keytag, ind)\n",
    "                        raise IndexError\n",
    "\n",
    "                    found_lbt = lbt\n",
    "                    found_let = let\n",
    "                    found_n = n\n",
    "                    #print(found_lbt)\n",
    "                    content = \" \".join(self.linelist[ind:close])\n",
    "                    break\n",
    "                \n",
    "        return found_n, content\n",
    "                \n",
    "                \n",
    "    def find_closing(self, closetag, startindex):\n",
    "        if closetag == '}':\n",
    "            brace_count = 0\n",
    "            while startindex < self.tot_lines:\n",
    "                #print(\"startindex: \", startindex)\n",
    "                if self.linelist[startindex].find('{') != -1:\n",
    "                    brace_count += 1\n",
    "                if self.linelist[startindex].find('}') != -1:\n",
    "                    brace_count -= 1\n",
    "                    if brace_count == 0:\n",
    "                        return startindex + 1\n",
    "                    else:\n",
    "                        startindex += 1\n",
    "                else:\n",
    "                    startindex += 1\n",
    "        else:    \n",
    "            while startindex < self.tot_lines:\n",
    "                if self.linelist[startindex].find(closetag) != -1:\n",
    "                    return startindex + 1\n",
    "                else:\n",
    "                    startindex += 1\n",
    "\n",
    "            \n",
    "         \n",
    "\n",
    "class parsetree:\n",
    "    \"\"\" treelike structure, contains nodes\n",
    "    \n",
    "    This parsetree will later be converted to JSON,\n",
    "    For nor it contains lists with childnodes at each node, each of these childnodes is a lower level section\n",
    "    This will be converted to the json format later (with perhaps an intermediary dictionary format)\n",
    "    \"\"\"\n",
    "    def __init__(self, tex_dir, json_dir, subdir, documentID, overwrite=False, _print=False):\n",
    "        \"\"\" \n",
    "\n",
    "        :param linelist: list of lines from a latex file, stripped from comments and everything before \n",
    "            the \\makefile tag, as \\newcommand shenanigans make life difficult otherwise.\n",
    "        :param documentID: string, the filename\n",
    "        \"\"\"\n",
    "        latex_lines = []\n",
    "        tfname = os.path.join(os.path.join(tex_dir, subdir), documentID)\n",
    "        jsubdir = os.path.join(json_dir, subdir)\n",
    "        jfname = os.path.join(jsubdir, documentID)\n",
    "        \n",
    "        if os.path.exists(jfname):\n",
    "            if not overwrite:\n",
    "                print(\"already parsed, going to next\")\n",
    "                return \n",
    "        else:\n",
    "            if not os.path.exists(jsubdir):\n",
    "                os.mkdir(jsubdir)\n",
    "\n",
    "\n",
    "        with open(tfname, 'r', encoding='utf-8') as f:\n",
    "            latex_lines = f.readlines()\n",
    "\n",
    "        #no_comments = strip_comments(latex_lines)\n",
    "        #self.headnode = Node(documentID, no_comments, 0, headnode=True)\n",
    "        self.headnode = Node(documentID, latex_lines, 0, headnode=True)\n",
    "        JSON = JSONify(self.headnode)\n",
    "        \n",
    "    \n",
    "        with open(jfname, 'w') as f:\n",
    "            print(jfname)\n",
    "            #print(str(json.dumps(JSON)))\n",
    "            f.write(str(json.dumps(JSON)))\n",
    "            raise KeyboardInterrupt\n",
    "        \n",
    "def JSON_unknown_cn(node):\n",
    "    if type(node) == str:\n",
    "        return node.translate(PUNCTUATION_TABLE) #remove punctuation\n",
    "    else:\n",
    "        return {node.name : [JSON_unknown_cn(cn) for cn in node.cn]}\n",
    "\n",
    "def JSONify(node):\n",
    "    base = {node.name : JSON_unknown_cn(node)}\n",
    "\n",
    "    if node.headnode:\n",
    "        base.update(node.other_keys)\n",
    "\n",
    "    return base\n",
    "\n",
    "def JSONify_str(node):\n",
    "    return node.cn\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "# KEYTAGS have format [\"name\", [[\"latex_begin_tag_option1\", \"latex_end_tag_option1\"], [\"latex_begin_tag_option1\", \"latex_end_tag_option1\"]]]\n",
    "KEYTAGS = [[\"date\",[[\"\\date{\", \"}\"]]],\n",
    "           [\"abstract\",[[\"\\\\begin{abstract}\", \"\\end{abstract}\"], [\"\\section{abstract}\", \"\\section{}\"]]], \n",
    "           [\"keywords\", [[\"{\\\\bf Key words:}\", \".\"]]],\n",
    "           [\"author\", [[\"\\\\author{\", \"}\"]]],\n",
    "           [\"keywords\",[[\"\\\\it Key words:\",\"\\end\"]]],\n",
    "           [\"content\", [[\"\\\\section{\", \"\\end{document}\"]]],\n",
    "           [\"introduction\", [[\"\\\\newsec{introduction\", \"\\\\newsec\"],   # includes first line of section after intro\n",
    "                             [\"\\\\section{introduction\", \"\\\\section{\"] # includes first line of section after intro\n",
    "                    ]\n",
    "                ]\n",
    "          ]\n",
    "\n",
    "# KEYTAGS have format [\"name\", [\"latex_begin_tag\", \"latex_end_tag\"]]\n",
    "KEYTAGS = [[\"date\",[[\"\\date{\", \"}\"], [\"%Date: \", \"\\n\"]]],\n",
    "           [\"abstract\",[[\"\\\\begin{abstract}\", \"\\end{abstract}\"]]], \n",
    "           [\"keywords\", [[\"{\\\\bf Key words:}\", \".\"]]],\n",
    "           [\"author\", [[\"\\\\author{\", \"}\"], [\"%From: \", \"\\n\"]]],\n",
    "           [\"keywords\",[[\"\\\\it Key words:\",\"\\end\"]]],\n",
    "           [\"content\", [[\"\\\\section{\", \"\\end{document}\"]]],\n",
    "           [\"introduction\", [[\"\\\\newsec{\", \"\\\\newsec\"],   # includes first line of section after intro\n",
    "                             [\"\\\\section{\", \"\\\\section{\"] # includes first line of section after intro\n",
    "                    ]\n",
    "                ]\n",
    "          ]\n",
    "\n",
    "\n",
    "# KEYTAGS have format [\"name\", [\"latex_begin_tag\", \"latex_end_tag\"]]\n",
    "KEYTAGS = [[\"date\",[[\"\\\\date{\", \"}\"], [\"%Date: \", \"\\n\"]]],\n",
    "           [\"abstract\",[[\"\\\\begin{abstract}\", \"\\\\end{abstract}\"], [\"\\\\abstract{\", \"}\"], [\"\\\\Abstract{\", \"}\"],\n",
    "                        [\"Abstract\", \"\\\\new\"], [\"abstract\", \"\\\\new\"], [\"abstract\", \"\\\\end\"], [\"Abstract\", \"\\\\end\"],\n",
    "                        [\"\\abstract{\", \"}\"],]],\n",
    "           [\"author\", [[\"\\\\author{\", \"}\"], [\"%From: \", \"\\n\"]]],\n",
    "           [\"keywords\",[[\"\\\\it Key words:\",\"\\\\end\"], [\"\\\\Key words:\",\"\\\\end\"], [\"Key words\", \".\"]]],\n",
    "           [\"content\", [[\"\\\\section{\", \"\\\\end{document}\"]]],\n",
    "           [\"introduction\", [[\"\\\\newsec{\", \"\\\\newsec\"],   # includes first line of section after intro\n",
    "                             [\"\\\\section{\", \"\\\\section{\"] # includes first line of section after intro\n",
    "                    ]\n",
    "                ]\n",
    "          ]\n",
    "\n",
    "PUNCTUATION_TABLE = str.maketrans({key: \" \" for key in string.punctuation})\n",
    "\n",
    "\n",
    "tex_dir = \"unzipped\"\n",
    "docID = \"0301005\"\n",
    "json_dir = \"json\"\n",
    "sub_dir = \"hep-th-2003\"\n",
    "doctree = parsetree(tex_dir, json_dir, sub_dir, docID, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = \"json\"\n",
    "tex_dir = \"unzipped\"\n",
    "\n",
    "faulty = []\n",
    "\n",
    "for sd in os.listdir(\"unzipped\"):\n",
    "    if not (sd == \"hep-th-1992\"):\n",
    "        continue\n",
    "    for file in os.listdir(os.path.join(tex_dir, sd)):\n",
    "        try:\n",
    "            parsetree(tex_dir, json_dir, sd, file, overwrite=True)\n",
    "        except UnicodeDecodeError:\n",
    "            faulty.append(file)\n",
    "print(faulty)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(faulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(' +', ' ', \"n$%^&*()ode!@#$%^&*(\".translate(PUNCTUATION_TABLE)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
